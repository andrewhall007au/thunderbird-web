---
phase: 09-monitoring-alerting
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/monitoring/__init__.py
  - backend/monitoring/config.py
  - backend/monitoring/storage.py
  - backend/monitoring/checks.py
  - backend/monitoring/scheduler.py
  - backend/monitoring/main.py
autonomous: true

must_haves:
  truths:
    - "Health checks run automatically on schedule and detect backend/frontend/database down"
    - "Check results are stored in SQLite metrics database with timestamps"
    - "Scheduler runs checks at configured intervals without manual intervention"
    - "Database query performance is tracked separately from basic connectivity"
    - "External API latency is tracked for Stripe, Twilio, and weather APIs"
  artifacts:
    - path: "backend/monitoring/storage.py"
      provides: "SQLite metrics storage with UUIDv7 keys, time series queries"
      contains: "CREATE TABLE.*metrics"
    - path: "backend/monitoring/checks.py"
      provides: "Health check implementations for backend, frontend, API, database query performance, and external API latency"
      exports: ["run_health_checks", "CheckResult", "check_database_query_performance", "check_external_api_latency"]
    - path: "backend/monitoring/scheduler.py"
      provides: "APScheduler setup with interval-based check scheduling"
      contains: "AsyncIOScheduler"
    - path: "backend/monitoring/main.py"
      provides: "FastAPI application for monitoring service"
      contains: "FastAPI"
    - path: "backend/monitoring/config.py"
      provides: "Monitoring configuration (URLs, intervals, thresholds)"
      contains: "PRODUCTION_URL"
  key_links:
    - from: "backend/monitoring/scheduler.py"
      to: "backend/monitoring/checks.py"
      via: "scheduled job calls check functions"
      pattern: "scheduler\\.add_job.*check"
    - from: "backend/monitoring/checks.py"
      to: "backend/monitoring/storage.py"
      via: "check results stored as metrics"
      pattern: "store_metric"
---

<objective>
Create the monitoring service foundation: metrics database, health check implementations (including database query performance and external API latency tracking), and scheduled check runner.

Purpose: This is the data layer and execution engine for all monitoring. Every subsequent plan (alerts, synthetic tests, dashboard) depends on this foundation storing check results in a queryable format.

Output: A standalone monitoring service (`backend/monitoring/`) with SQLite metrics DB, health checks for all critical systems including DB query performance and external API latency, and APScheduler running checks on intervals.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-monitoring-alerting/09-RESEARCH.md

# Existing monitoring script to evolve from
@backend/scripts/quick_monitor.py

# Backend structure to follow
@backend/app/main.py
@backend/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Monitoring config, metrics storage, and check result types</name>
  <files>
    backend/monitoring/__init__.py
    backend/monitoring/config.py
    backend/monitoring/storage.py
  </files>
  <action>
Create `backend/monitoring/` as a new Python package alongside `backend/app/`.

**backend/monitoring/__init__.py** - Empty init file.

**backend/monitoring/config.py** - Monitoring configuration using Pydantic BaseSettings:
- `PRODUCTION_URL`: str = "https://thunderbird.bot" (base URL to monitor)
- `MONITORING_DB_PATH`: str = "backend/monitoring/monitoring.db"
- `ALERT_PHONE_NUMBERS`: list[str] = [] (phone numbers for SMS alerts, E.164 format)
- `ALERT_EMAIL_ADDRESSES`: list[str] = [] (emails for warning alerts)
- `CHECK_INTERVALS`: dict mapping check names to interval minutes:
  - `health_check`: 1
  - `beta_signup_flow`: 5
  - `checkout_flow`: 15
  - `login_flow`: 10
  - `weather_api`: 10
  - `sms_webhook`: 1440 (daily)
  - `db_query_performance`: 5
  - `external_api_latency`: 10
- `CONSECUTIVE_FAILURES_BEFORE_ALERT`: int = 2 (require 2 consecutive failures before alerting)
- `SMS_RATE_LIMIT_PER_HOUR`: int = 10
- `METRICS_RETENTION_DAYS`: int = 90
- `DB_QUERY_SLOW_THRESHOLD_MS`: float = 500.0 (threshold for degraded DB query performance)
- `EXTERNAL_API_SLOW_THRESHOLD_MS`: float = 5000.0 (threshold for degraded external API latency)
- Import Twilio/Resend credentials from parent settings: `TWILIO_ACCOUNT_SID`, `TWILIO_AUTH_TOKEN`, `TWILIO_PHONE_NUMBER`, `RESEND_API_KEY`
- Load from environment variables with `MONITOR_` prefix where appropriate

**backend/monitoring/storage.py** - SQLite metrics storage:
- Use WAL mode for concurrent read/write: `PRAGMA journal_mode=WAL`
- Create `metrics` table:
  ```sql
  CREATE TABLE IF NOT EXISTS metrics (
      id TEXT PRIMARY KEY,          -- UUID string
      timestamp_ms INTEGER NOT NULL,
      check_name TEXT NOT NULL,
      status TEXT NOT NULL,         -- 'pass', 'fail', 'degraded'
      duration_ms REAL,
      error_message TEXT,
      metadata TEXT                 -- JSON for additional context
  );
  CREATE INDEX IF NOT EXISTS idx_metrics_ts ON metrics(timestamp_ms DESC);
  CREATE INDEX IF NOT EXISTS idx_metrics_check ON metrics(check_name, status, timestamp_ms DESC);
  ```
- Create `incidents` table:
  ```sql
  CREATE TABLE IF NOT EXISTS incidents (
      id TEXT PRIMARY KEY,
      check_name TEXT NOT NULL,
      severity TEXT NOT NULL,       -- 'critical', 'warning', 'info'
      status TEXT NOT NULL,         -- 'active', 'resolved', 'acknowledged'
      first_seen_ms INTEGER NOT NULL,
      last_seen_ms INTEGER NOT NULL,
      resolved_ms INTEGER,
      failure_count INTEGER DEFAULT 1,
      message TEXT,
      created_at TEXT DEFAULT CURRENT_TIMESTAMP
  );
  ```
- Functions:
  - `init_db()` - Create tables if not exist
  - `store_metric(check_name, status, duration_ms, error_message=None, metadata=None)` - Insert metric row with UUID and current timestamp
  - `get_recent_metrics(check_name, hours=1)` - Query recent metrics for a check
  - `get_uptime_stats(check_name, hours=24)` - Return total checks, pass count, avg duration
  - `get_all_latest_statuses()` - Return latest metric per check_name (for dashboard)
  - `get_consecutive_failures(check_name)` - Count consecutive recent failures (for alert dedup)
  - `cleanup_old_metrics(retention_days=90)` - Delete metrics older than retention period
  - `store_incident(check_name, severity, message)` - Create or update active incident
  - `resolve_incident(check_name)` - Mark incident as resolved
  - `acknowledge_incident(incident_id)` - Mark incident as acknowledged (set status='acknowledged')
  - `get_active_incidents()` - Return all active incidents
  - `get_incident_timeline(incident_id)` - Return incident with all associated metrics during its duration
- Use `uuid.uuid4()` for IDs (UUIDv7 not in stdlib, uuid4 is sufficient)
- Use `int(datetime.utcnow().timestamp() * 1000)` for timestamp_ms
- Connection uses `timeout=30` for lock waiting

Define `CheckResult` dataclass:
```python
@dataclass
class CheckResult:
    check_name: str
    status: str        # 'pass', 'fail', 'degraded'
    duration_ms: float
    error_message: str | None = None
    metadata: dict | None = None
```
  </action>
  <verify>
Run `python -c "from monitoring.config import MonitoringSettings; print('Config OK')"` from backend/ directory.
Run `python -c "from monitoring.storage import init_db, store_metric, get_recent_metrics; init_db(); store_metric('test', 'pass', 100.0); print(get_recent_metrics('test'))"` from backend/ directory.
  </verify>
  <done>Monitoring config loads from environment, metrics DB creates tables on init, metrics can be stored and queried, incidents can be acknowledged, CheckResult dataclass is importable.</done>
</task>

<task type="auto">
  <name>Task 2: Health checks (including DB query performance and external API latency) and APScheduler runner</name>
  <files>
    backend/monitoring/checks.py
    backend/monitoring/scheduler.py
    backend/monitoring/main.py
  </files>
  <action>
**backend/monitoring/checks.py** - Health check implementations:

Evolve from `quick_monitor.py` but structured as individual check functions that return `CheckResult`.

Implement these checks:
1. `check_backend_health()` - GET {PRODUCTION_URL}/health, expect 200, measure response time
2. `check_frontend_loads()` - GET {PRODUCTION_URL}, expect 200 and content length > 1000
3. `check_beta_signup_endpoint()` - POST {PRODUCTION_URL}/api/beta/apply with empty validation data, expect 400/422 (NOT 404/500)
4. `check_api_response_time()` - GET {PRODUCTION_URL}/health, check response time < 2.0 seconds, return 'degraded' if between 1.0-2.0s
5. `check_database_health()` - GET {PRODUCTION_URL}/health (backend health already checks DB connectivity)
6. `check_weather_api()` - POST {PRODUCTION_URL}/api/routes/forecast-preview with test coordinates, expect 200/400/422

7. `check_database_query_performance()` - Test database query performance separately from basic connectivity (MON-03):
   - GET {PRODUCTION_URL}/health/detailed (or a custom endpoint if the backend has one)
   - Alternatively, use the monitoring service's own connection to the production database:
     - Run a representative query: `SELECT COUNT(*) FROM accounts` (simple aggregate)
     - Run a join query: `SELECT COUNT(*) FROM orders o JOIN accounts a ON o.account_id = a.id WHERE o.created_at > datetime('now', '-7 days')` (representative of typical load)
     - Measure query execution time for each
   - Return 'pass' if both queries complete under DB_QUERY_SLOW_THRESHOLD_MS (500ms)
   - Return 'degraded' if queries complete but exceed threshold
   - Return 'fail' if queries error or timeout
   - Include query times in metadata: `{"simple_query_ms": X, "join_query_ms": Y}`
   - Note: The monitoring service needs read-only access to the production SQLite database. Configure `PRODUCTION_DB_PATH` in config (default: `/root/overland-weather/backend/production.db` or wherever the app DB lives).

8. `check_external_api_latency()` - Test latency to external APIs that Thunderbird depends on (MON-03):
   - **Stripe**: GET `https://api.stripe.com/v1/balance` with Stripe secret key, measure response time. If no key available, GET `https://api.stripe.com/` and check for 200/401 (just testing reachability).
   - **Twilio**: GET `https://api.twilio.com/2010-04-01/Accounts/{ACCOUNT_SID}.json` with basic auth, measure response time. If no creds, GET `https://api.twilio.com/` for reachability.
   - **Weather API (Open-Meteo)**: GET `https://api.open-meteo.com/v1/forecast?latitude=0&longitude=0&hourly=temperature_2m&forecast_days=1`, measure response time.
   - Return a CheckResult per API, or a single combined CheckResult:
     - `check_name`: "external_api_latency"
     - `status`: 'pass' if all APIs respond under EXTERNAL_API_SLOW_THRESHOLD_MS (5s), 'degraded' if any slow, 'fail' if any unreachable
     - `metadata`: `{"stripe_ms": X, "twilio_ms": Y, "openmeteo_ms": Z}`
   - Each API call uses `timeout=10` seconds

Each check:
- Uses `requests` with `timeout=15`
- Wraps in try/except, returns CheckResult with status='fail' on any exception
- Measures duration using `time.monotonic()`
- Returns structured CheckResult

Add `run_all_health_checks() -> list[CheckResult]` that runs all checks and stores results via `store_metric()`.

**backend/monitoring/scheduler.py** - APScheduler configuration:

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
```

- `create_scheduler() -> AsyncIOScheduler`:
  - Configure with `timezone='UTC'`, `coalesce=True`, `max_instances=1`, `misfire_grace_time=300`
  - Add jobs based on `CHECK_INTERVALS` from config:
    - `health_check` (1 min): runs `check_backend_health`, `check_frontend_loads`, `check_api_response_time`
    - `beta_signup_flow` (5 min): runs `check_beta_signup_endpoint` (placeholder - Plan 03 adds Playwright)
    - `weather_api` (10 min): runs `check_weather_api`
    - `db_query_performance` (5 min): runs `check_database_query_performance`
    - `external_api_latency` (10 min): runs `check_external_api_latency`
  - Add cleanup job: daily at 3 AM UTC, runs `cleanup_old_metrics`
  - Add listener for job errors (log them)
  - Each scheduled job: run check -> store_metric -> return result (alert evaluation added in Plan 02)

**backend/monitoring/main.py** - FastAPI monitoring service:

Create a minimal FastAPI app that:
- On startup: `init_db()`, create and start scheduler
- On shutdown: gracefully shutdown scheduler
- GET `/` - Returns monitoring service status (uptime, scheduler running, job count)
- GET `/health` - Returns 200 with `{"status": "healthy", "scheduler_running": true/false}`
- GET `/api/metrics/latest` - Returns latest status for all checks (calls `get_all_latest_statuses()`)
- GET `/api/metrics/{check_name}?hours=24` - Returns recent metrics for a check
- GET `/api/uptime?hours=24` - Returns uptime stats for all checks
- GET `/api/incidents` - Returns active incidents

Run on port 8001 (separate from main app on 8000).

Add `if __name__ == "__main__"` block:
```python
import uvicorn
uvicorn.run("monitoring.main:app", host="0.0.0.0", port=8001, reload=True)
```
  </action>
  <verify>
From backend/ directory:
1. `python -c "from monitoring.checks import check_backend_health, check_database_query_performance, check_external_api_latency, CheckResult; print('Checks import OK')"` succeeds
2. `python -c "from monitoring.scheduler import create_scheduler; s = create_scheduler(); print(f'Scheduler created with {len(s.get_jobs())} jobs')"` succeeds
3. `python -c "from monitoring.main import app; print(f'App routes: {[r.path for r in app.routes]}')"` shows expected routes
  </verify>
  <done>Health checks detect backend/frontend/API failures. Database query performance tracked separately with slow-query threshold. External API latency tracked for Stripe, Twilio, and Open-Meteo. Scheduler runs checks at configured intervals. Monitoring FastAPI app serves metrics via API on port 8001. All check results stored in SQLite metrics DB.</done>
</task>

</tasks>

<verification>
- `backend/monitoring/` directory exists with __init__.py, config.py, storage.py, checks.py, scheduler.py, main.py
- `python -m monitoring.main` starts the monitoring service on port 8001
- GET /health returns 200 with scheduler status
- GET /api/metrics/latest returns latest check statuses
- Metrics database creates and stores check results
- Scheduler has jobs configured at correct intervals including db_query_performance and external_api_latency
- Database query performance check returns structured results with query times
- External API latency check tests Stripe, Twilio, and Open-Meteo reachability and speed
- Incident acknowledgment works via storage function
</verification>

<success_criteria>
- Monitoring service starts independently on port 8001
- Health checks run against production URL and return structured results
- Database query performance tracked every 5 minutes with slow-query detection
- External API latency tracked every 10 minutes for Stripe, Twilio, Open-Meteo
- Results stored in SQLite with timestamps and queryable by check name
- APScheduler runs checks at configured intervals (1min health, 5min beta+db, 10min weather+external)
- Metrics API endpoints serve check history and uptime stats
- Incident tracking (create/resolve/acknowledge) works
</success_criteria>

<output>
After completion, create `.planning/phases/09-monitoring-alerting/09-01-SUMMARY.md`
</output>
