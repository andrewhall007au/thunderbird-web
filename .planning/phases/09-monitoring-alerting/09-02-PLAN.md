---
phase: 09-monitoring-alerting
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - backend/monitoring/alerts/__init__.py
  - backend/monitoring/alerts/manager.py
  - backend/monitoring/alerts/channels.py
  - backend/monitoring/scheduler.py
autonomous: true

must_haves:
  truths:
    - "Critical failures trigger immediate SMS alert to configured phone numbers"
    - "Warning alerts send email, escalate to SMS after 15 minutes unresolved"
    - "Duplicate alerts are suppressed - same check failure does not spam SMS"
    - "Recovery notifications are sent when a failing check passes again"
    - "SMS rate is limited to prevent cost explosion"
  artifacts:
    - path: "backend/monitoring/alerts/manager.py"
      provides: "Alert evaluation, deduplication, escalation logic"
      exports: ["AlertManager", "evaluate_and_alert"]
      contains: "consecutive_failures"
    - path: "backend/monitoring/alerts/channels.py"
      provides: "SMS (Twilio) and Email (Resend) alert channels"
      exports: ["TwilioSMSChannel", "ResendEmailChannel"]
      contains: "twilio"
  key_links:
    - from: "backend/monitoring/scheduler.py"
      to: "backend/monitoring/alerts/manager.py"
      via: "scheduler calls evaluate_and_alert after each check"
      pattern: "evaluate_and_alert"
    - from: "backend/monitoring/alerts/manager.py"
      to: "backend/monitoring/alerts/channels.py"
      via: "severity routing to SMS or email"
      pattern: "sms_channel\\.send|email_channel\\.send"
    - from: "backend/monitoring/alerts/manager.py"
      to: "backend/monitoring/storage.py"
      via: "reads consecutive failures, creates/resolves incidents"
      pattern: "get_consecutive_failures|store_incident|resolve_incident"
---

<objective>
Build the alert management system with SMS and email channels, deduplication, severity routing, and escalation.

Purpose: Transform check results into actionable alerts. Without this, checks run but nobody is notified when things break. This is the most critical piece for catching issues like the BetaApplyModal bug.

Output: AlertManager that evaluates check results, deduplicates alerts, routes to SMS/email based on severity, and sends recovery notifications.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/09-monitoring-alerting/09-RESEARCH.md
@.planning/phases/09-monitoring-alerting/09-01-SUMMARY.md

# Existing Twilio usage for reference
@backend/scripts/quick_monitor.py

# Settings with Twilio/Resend credentials
@backend/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Alert channels (SMS via Twilio, Email via Resend)</name>
  <files>
    backend/monitoring/alerts/__init__.py
    backend/monitoring/alerts/channels.py
  </files>
  <action>
Create `backend/monitoring/alerts/` package.

**backend/monitoring/alerts/__init__.py** - Empty init.

**backend/monitoring/alerts/channels.py** - Alert delivery channels:

`TwilioSMSChannel` class:
- `__init__(account_sid, auth_token, from_number)` - Store Twilio credentials
- `send(message: str, to_numbers: list[str]) -> bool` - Send SMS to all configured numbers
  - Use `twilio.rest.Client` to send SMS
  - Keep messages under 160 chars for single segment (cost control)
  - Log success/failure for each number
  - Return True if at least one SMS sent successfully
- `_format_alert_sms(severity: str, check_name: str, message: str) -> str` - Format for SMS:
  ```
  THUNDERBIRD {SEVERITY}: {check_name}
  {message}
  {timestamp}
  ```
  Truncate to 155 chars max.
- `_format_recovery_sms(check_name: str, downtime_minutes: int) -> str`:
  ```
  THUNDERBIRD OK: {check_name} recovered
  Downtime: {downtime_minutes}min
  {timestamp}
  ```

`ResendEmailChannel` class:
- `__init__(api_key, from_email="alerts@thunderbird.bot")` - Store Resend credentials
- `send(alert_subject: str, alert_html: str, to_emails: list[str]) -> bool` - Send email
  - Use `resend.Emails.send()`
  - Format as HTML with severity color coding (red for critical, orange for warning, blue for info)
  - Include: check name, severity, error message, timestamp, link to /monitoring dashboard
  - Respect 2 req/sec rate limit (add 0.5s sleep between sends if multiple recipients)
  - Return True if sent successfully
- `_format_alert_html(severity, check_name, message, first_seen, failure_count) -> str` - HTML email template with:
  - Color-coded severity banner (critical=red, warning=orange)
  - Check name and error message
  - First failure timestamp
  - Number of consecutive failures
  - Link to monitoring dashboard

Both channels:
- Handle missing credentials gracefully (log warning, don't crash)
- Log all send attempts (success and failure) with timestamps
- Catch and log all exceptions (never crash the monitoring service)
  </action>
  <verify>
`python -c "from monitoring.alerts.channels import TwilioSMSChannel, ResendEmailChannel; print('Channels import OK')"` from backend/ directory succeeds.
  </verify>
  <done>SMS and email alert channels are implemented, handle credentials gracefully, format messages appropriately for each medium, and never crash on send failures.</done>
</task>

<task type="auto">
  <name>Task 2: Alert manager with deduplication and escalation, wire into scheduler</name>
  <files>
    backend/monitoring/alerts/manager.py
    backend/monitoring/scheduler.py
  </files>
  <action>
**backend/monitoring/alerts/manager.py** - Core alert logic:

`AlertManager` class:
- `__init__(config, storage, sms_channel, email_channel)` - Initialize with dependencies
- Internal state:
  - `_last_alert_time: dict[str, datetime]` - Track last alert time per check (for dedup)
  - `_sms_count_this_hour: int` - Track SMS sends for rate limiting
  - `_hour_start: datetime` - When current hour started (reset counter)

- `async evaluate_and_alert(result: CheckResult)`:
  1. Store metric via storage.store_metric()
  2. If result.status == 'pass':
     - Check if there was an active incident for this check
     - If yes: resolve incident, send recovery notification (SMS for critical, email for warning)
     - Return
  3. If result.status == 'fail':
     - Get consecutive failure count from storage
     - If consecutive failures < CONSECUTIVE_FAILURES_BEFORE_ALERT (default 2): log and return (don't alert yet)
     - Determine severity based on check_name mapping:
       - Critical (immediate SMS + email): `health_check`, `beta_signup_flow`, `checkout_flow`
       - Warning (email, SMS after 15min): `login_flow`, `weather_api`, `api_response_time`
       - Info (log only): everything else
     - Check deduplication: if alert sent for this check in last 15 minutes, skip
     - Check SMS rate limit: if >= SMS_RATE_LIMIT_PER_HOUR, only send email
     - Store/update incident
     - Route to appropriate channel based on severity

- `_should_alert(check_name: str) -> bool`:
  - Check if last alert for this check was > 15 minutes ago
  - If no previous alert, return True

- `_can_send_sms() -> bool`:
  - Reset counter if hour changed
  - Return sms_count_this_hour < SMS_RATE_LIMIT_PER_HOUR

- `_determine_severity(check_name: str) -> str`:
  - Map check names to severity levels (configurable via config)
  - Default: critical for health/beta/checkout, warning for others

- `_send_critical_alert(check_name, result, incident)`:
  - Send SMS to all configured numbers (if can_send_sms)
  - Send email to all configured addresses
  - Increment SMS counter
  - Update last_alert_time

- `_send_warning_alert(check_name, result, incident)`:
  - Send email immediately
  - If incident.first_seen > 15 minutes ago AND no SMS sent: escalate to SMS

- `_send_recovery(check_name, incident)`:
  - Calculate downtime from incident first_seen to now
  - Send SMS recovery if original was critical
  - Send email recovery for all severities

Create module-level factory:
```python
def create_alert_manager(config, storage):
    sms = TwilioSMSChannel(config.TWILIO_ACCOUNT_SID, config.TWILIO_AUTH_TOKEN, config.TWILIO_PHONE_NUMBER)
    email = ResendEmailChannel(config.RESEND_API_KEY)
    return AlertManager(config, storage, sms, email)
```

**Update backend/monitoring/scheduler.py**:
- Import AlertManager and create_alert_manager
- After each check runs and stores metric, call `alert_manager.evaluate_and_alert(result)`
- The scheduler's check functions now:
  1. Run the check (get CheckResult)
  2. Call evaluate_and_alert(result) which both stores metric AND evaluates for alerts
- Remove the separate store_metric call from scheduler (alert manager handles it)
  </action>
  <verify>
`python -c "from monitoring.alerts.manager import AlertManager, create_alert_manager; print('AlertManager import OK')"` from backend/ directory.
Verify scheduler.py imports and calls evaluate_and_alert: `grep -n 'evaluate_and_alert' backend/monitoring/scheduler.py` shows integration.
  </verify>
  <done>Alert manager evaluates check results with deduplication (15-min window), requires 2 consecutive failures before alerting, routes critical alerts to SMS+email and warnings to email with 15-min escalation, sends recovery notifications, and is wired into the scheduler's check execution loop.</done>
</task>

</tasks>

<verification>
- AlertManager correctly suppresses first failure (requires 2 consecutive)
- Critical checks (health, beta, checkout) route to SMS + email
- Warning checks (login, weather) route to email with SMS escalation
- Deduplication prevents same alert within 15 minutes
- SMS rate limit (10/hour) prevents cost explosion
- Recovery notifications sent when failing check passes
- Scheduler calls evaluate_and_alert after every check
</verification>

<success_criteria>
- Critical failure detected -> SMS sent within 30 seconds to configured numbers
- Same failure persisting -> no duplicate SMS for 15 minutes
- Warning failure persisting 15+ minutes -> escalated from email to SMS
- Check recovers -> recovery SMS/email sent with downtime duration
- SMS rate limited to max 10/hour even during major outage
- All alert send attempts logged with timestamps
</success_criteria>

<output>
After completion, create `.planning/phases/09-monitoring-alerting/09-02-SUMMARY.md`
</output>
