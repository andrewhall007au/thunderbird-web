---
phase: 09-monitoring-alerting
plan: 06
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - backend/monitoring/logs/__init__.py
  - backend/monitoring/logs/collector.py
  - backend/monitoring/logs/storage.py
  - backend/monitoring/logs/analyzer.py
  - backend/monitoring/api.py
  - backend/monitoring/scheduler.py
autonomous: true

must_haves:
  truths:
    - "Application errors are centrally collected and stored in the monitoring database"
    - "Errors can be searched and filtered by severity, source, time range, and message content"
    - "Error rate is tracked over time and alerts trigger when rate exceeds threshold"
    - "Basic error pattern detection identifies recurring errors"
  artifacts:
    - path: "backend/monitoring/logs/collector.py"
      provides: "Log collection from Python logging and log files"
      exports: ["LogCollector", "collect_recent_errors"]
      contains: "logging"
    - path: "backend/monitoring/logs/storage.py"
      provides: "SQLite storage for centralized error logs with search capabilities"
      exports: ["init_log_tables", "store_log_entry", "search_logs", "get_error_rate"]
      contains: "CREATE TABLE.*error_logs"
    - path: "backend/monitoring/logs/analyzer.py"
      provides: "Error pattern detection and rate tracking"
      exports: ["detect_error_patterns", "check_error_rate"]
      contains: "pattern"
    - path: "backend/monitoring/api.py"
      provides: "API endpoints for log search, error rate, and patterns (added to existing router)"
      contains: "search_logs"
  key_links:
    - from: "backend/monitoring/logs/collector.py"
      to: "backend/monitoring/logs/storage.py"
      via: "collector stores parsed log entries"
      pattern: "store_log_entry"
    - from: "backend/monitoring/logs/analyzer.py"
      to: "backend/monitoring/logs/storage.py"
      via: "analyzer queries stored logs for patterns"
      pattern: "search_logs|get_error_rate"
    - from: "backend/monitoring/scheduler.py"
      to: "backend/monitoring/logs/collector.py"
      via: "scheduled job collects logs periodically"
      pattern: "collect_recent_errors"
    - from: "backend/monitoring/api.py"
      to: "backend/monitoring/logs/storage.py"
      via: "API endpoints query log storage"
      pattern: "search_logs|get_error_rate|detect_error_patterns"
---

<objective>
Build centralized error log aggregation with search, error rate tracking, and pattern detection.

Purpose: MON-07 requires centralized error logging, search/filter capabilities, error rate tracking, and pattern detection. Currently no plan addresses this. Without log aggregation, debugging production issues requires SSH into the server and manually reading log files. This plan creates a lightweight log collection and analysis system within the existing monitoring service.

Output: Log collector that ingests errors from the backend application, stores them in SQLite with full-text search, tracks error rates with alerting, and detects recurring error patterns.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/09-monitoring-alerting/09-RESEARCH.md
@.planning/phases/09-monitoring-alerting/09-01-SUMMARY.md

# Backend logging patterns
@backend/app/main.py
@backend/config/settings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Log storage schema, collector, and error pattern analyzer</name>
  <files>
    backend/monitoring/logs/__init__.py
    backend/monitoring/logs/collector.py
    backend/monitoring/logs/storage.py
    backend/monitoring/logs/analyzer.py
  </files>
  <action>
Create `backend/monitoring/logs/` package for centralized error log aggregation.

**backend/monitoring/logs/__init__.py** - Empty init.

**backend/monitoring/logs/storage.py** - SQLite storage for error logs:

Use the same monitoring.db database (from monitoring/storage.py). Add new tables:

```sql
CREATE TABLE IF NOT EXISTS error_logs (
    id TEXT PRIMARY KEY,
    timestamp_ms INTEGER NOT NULL,
    level TEXT NOT NULL,          -- 'ERROR', 'WARNING', 'CRITICAL'
    source TEXT NOT NULL,         -- module/file that generated the error (e.g., 'backend.app.routers.webhook')
    message TEXT NOT NULL,        -- error message
    traceback TEXT,               -- full traceback if available
    request_path TEXT,            -- HTTP request path if available (e.g., '/api/beta/apply')
    request_method TEXT,          -- HTTP method (GET, POST, etc.)
    metadata TEXT                 -- JSON for additional context (user_id, request_id, etc.)
);
CREATE INDEX IF NOT EXISTS idx_error_logs_ts ON error_logs(timestamp_ms DESC);
CREATE INDEX IF NOT EXISTS idx_error_logs_level ON error_logs(level, timestamp_ms DESC);
CREATE INDEX IF NOT EXISTS idx_error_logs_source ON error_logs(source, timestamp_ms DESC);
```

```sql
CREATE TABLE IF NOT EXISTS error_patterns (
    id TEXT PRIMARY KEY,
    pattern_hash TEXT NOT NULL UNIQUE,  -- hash of normalized error message for dedup
    first_seen_ms INTEGER NOT NULL,
    last_seen_ms INTEGER NOT NULL,
    occurrence_count INTEGER DEFAULT 1,
    sample_message TEXT NOT NULL,       -- representative error message
    source TEXT NOT NULL,               -- module that generates this error
    severity TEXT DEFAULT 'unknown',    -- auto-classified or manual
    status TEXT DEFAULT 'new'           -- 'new', 'known', 'resolved', 'ignored'
);
CREATE INDEX IF NOT EXISTS idx_error_patterns_hash ON error_patterns(pattern_hash);
CREATE INDEX IF NOT EXISTS idx_error_patterns_count ON error_patterns(occurrence_count DESC);
```

Functions:
- `init_log_tables()` - Create tables if not exist (called from main init_db)
- `store_log_entry(level, source, message, traceback=None, request_path=None, request_method=None, metadata=None)` - Insert log entry with UUID and timestamp
- `search_logs(query=None, level=None, source=None, start_ms=None, end_ms=None, limit=100, offset=0) -> list[dict]` - Search and filter logs:
  - `query`: substring search on message and traceback (use SQL LIKE %query%)
  - `level`: filter by level ('ERROR', 'WARNING', 'CRITICAL')
  - `source`: filter by source module (exact match or prefix match with *)
  - `start_ms`/`end_ms`: time range filter
  - Returns list of log entries sorted by timestamp DESC
  - Include total count for pagination
- `get_error_rate(hours=1) -> dict` - Calculate error rate:
  - Count errors in the period
  - Group by source and level
  - Calculate errors per minute
  - Return: `{"total_errors": N, "errors_per_minute": X, "by_source": {...}, "by_level": {...}}`
- `get_error_count_by_interval(hours=24, interval_minutes=60) -> list[dict]` - Time-bucketed error counts for charting
- `store_error_pattern(pattern_hash, message, source)` - Create or update error pattern (increment count, update last_seen)
- `get_top_patterns(limit=20) -> list[dict]` - Most frequent error patterns
- `cleanup_old_logs(retention_days=90)` - Delete logs older than retention

**backend/monitoring/logs/collector.py** - Log collection:

Two collection strategies:

1. **Python logging handler** (recommended for new errors):
   `MonitoringLogHandler(logging.Handler)`:
   - Custom logging handler that can be added to the backend app's logger
   - `emit(record)`: Convert LogRecord to stored log entry
     - Extract: level, source (logger name), message, traceback (if exc_info), request context
     - Call `store_log_entry()` to persist
     - Only capture ERROR and above (configurable threshold)
   - This handler is added to the root logger or specific loggers in the backend app

2. **Log file scraping** (for existing log output):
   `collect_recent_errors(log_file_path: str, since_ms: int = None) -> list[dict]`:
   - Read the production log file (e.g., `/var/log/thunderbird-backend.log` or systemd journal output)
   - Parse log lines matching error patterns:
     - Python tracebacks: lines starting with "Traceback" through the error line
     - FastAPI/uvicorn errors: lines containing "ERROR" level
     - Application errors: lines matching `ERROR|CRITICAL|FATAL`
   - Extract: timestamp, level, source, message, full traceback
   - Filter to only entries after `since_ms` (track last scrape position)
   - Return parsed log entries
   - Store each entry via `store_log_entry()`

   `collect_from_systemd_journal(service_name: str = "thunderbird-backend", since_minutes: int = 5) -> list[dict]`:
   - Alternative collector using `journalctl -u {service_name} --since "{since_minutes} min ago" -p err --no-pager -o json`
   - Parse JSON output from journalctl
   - Extract: timestamp, priority/level, message, unit
   - Store entries via `store_log_entry()`
   - This is more reliable than file scraping on systemd-based servers

Both collectors:
- Track last collection timestamp to avoid re-processing
- Handle file not found / permission errors gracefully
- Deduplicate entries by content hash within time window (same error within 1 second = single entry)

**backend/monitoring/logs/analyzer.py** - Error pattern detection:

`detect_error_patterns(storage, hours=24) -> list[dict]`:
- Query recent errors from storage
- Normalize error messages: strip timestamps, line numbers, UUIDs, request IDs, variable data
  - Replace UUIDs with `{UUID}`
  - Replace numbers with `{N}`
  - Replace quoted strings with `{STR}`
  - Replace IP addresses with `{IP}`
- Hash normalized messages to identify patterns
- Call `store_error_pattern()` for each unique pattern
- Return list of patterns with occurrence counts, sorted by frequency

`check_error_rate(storage, config) -> CheckResult`:
- Calculate current error rate (errors per minute over last 5 minutes)
- Compare against thresholds:
  - Normal: < 1 error/minute -> 'pass'
  - Elevated: 1-5 errors/minute -> 'degraded'
  - Critical: > 5 errors/minute -> 'fail'
- Return CheckResult with check_name="error_rate", status, metadata with rate details
- This check integrates with the existing alert pipeline (evaluate_and_alert)

`get_pattern_summary(storage) -> dict`:
- Return summary of error patterns:
  - New patterns (first seen in last 24h): likely new bugs
  - Trending patterns (count increasing): getting worse
  - Top patterns by frequency
  - Patterns by source module
  </action>
  <verify>
1. `python -c "from monitoring.logs.storage import init_log_tables, store_log_entry, search_logs, get_error_rate; print('Log storage OK')"` from backend/
2. `python -c "from monitoring.logs.collector import MonitoringLogHandler, collect_recent_errors; print('Collector OK')"` from backend/
3. `python -c "from monitoring.logs.analyzer import detect_error_patterns, check_error_rate; print('Analyzer OK')"` from backend/
  </verify>
  <done>Error log storage with search/filter capabilities. Log collector via Python handler and log file scraping. Error pattern detection with normalized message deduplication. Error rate tracking with threshold-based alerting.</done>
</task>

<task type="auto">
  <name>Task 2: Log aggregation API endpoints and scheduler integration</name>
  <files>
    backend/monitoring/api.py
    backend/monitoring/scheduler.py
  </files>
  <action>
**Update backend/monitoring/api.py** - Add log aggregation endpoints to existing API router:

1. `GET /api/monitoring/logs?query=&level=&source=&hours=24&limit=100&offset=0` - Search and filter logs:
   ```json
   {
     "total": 250,
     "limit": 100,
     "offset": 0,
     "logs": [
       {
         "id": "uuid",
         "timestamp": "2026-02-04T12:00:00Z",
         "level": "ERROR",
         "source": "backend.app.routers.webhook",
         "message": "Failed to process SMS from +1234567890",
         "traceback": "...",
         "request_path": "/api/webhook/sms",
         "request_method": "POST"
       }
     ]
   }
   ```
   - Support all filter parameters from search_logs()
   - Default to last 24 hours if no time range specified
   - Paginate with limit/offset

2. `GET /api/monitoring/logs/rate?hours=1` - Error rate over time:
   ```json
   {
     "period_hours": 1,
     "total_errors": 15,
     "errors_per_minute": 0.25,
     "by_level": {"ERROR": 12, "CRITICAL": 3},
     "by_source": {"backend.app.routers.webhook": 8, "backend.app.services.weather": 7},
     "trend": [
       {"interval_start": "2026-02-04T11:00:00Z", "count": 3},
       {"interval_start": "2026-02-04T11:15:00Z", "count": 5},
       ...
     ]
   }
   ```
   - Calls get_error_rate() and get_error_count_by_interval()

3. `GET /api/monitoring/logs/patterns?hours=24` - Error patterns:
   ```json
   {
     "patterns": [
       {
         "id": "uuid",
         "sample_message": "Failed to process SMS from {STR}",
         "source": "backend.app.routers.webhook",
         "occurrence_count": 45,
         "first_seen": "2026-02-01T10:00:00Z",
         "last_seen": "2026-02-04T12:00:00Z",
         "status": "new"
       }
     ],
     "new_patterns_24h": 2,
     "trending_patterns": 1
   }
   ```
   - Calls detect_error_patterns() and get_pattern_summary()

4. `PATCH /api/monitoring/logs/patterns/{pattern_id}` - Update pattern status:
   ```json
   // Request:
   { "status": "known" }  // 'known', 'resolved', 'ignored'

   // Response:
   { "id": "uuid", "status": "known" }
   ```
   - Allows marking patterns as known/resolved/ignored to reduce noise

**Update backend/monitoring/scheduler.py**:

Add log aggregation jobs:

1. `log_collection` (every 2 minutes):
   - Calls `collect_recent_errors()` or `collect_from_systemd_journal()` to ingest new errors
   - Uses the file scraping or journalctl approach depending on environment detection
   - On production (systemd available): use journalctl
   - On development: skip or use log file if configured

2. `error_rate_check` (every 5 minutes):
   - Calls `check_error_rate()` from analyzer
   - Feeds result through `evaluate_and_alert()` pipeline
   - Elevated error rate triggers warning alert
   - Critical error rate triggers critical alert (SMS)

3. `pattern_detection` (every 30 minutes):
   - Calls `detect_error_patterns()` to update pattern database
   - If new patterns detected (first_seen in last 30 min), store metric as info

Add `init_log_tables()` call to the monitoring service startup in main.py (alongside existing `init_db()`).
  </action>
  <verify>
1. `python -c "from monitoring.api import router; routes = [r.path for r in router.routes]; print(f'Routes: {routes}'); assert '/api/monitoring/logs' in str(routes) or 'logs' in str(routes)"` from backend/ shows log endpoints.
2. `grep -n 'log_collection\|error_rate_check\|pattern_detection' backend/monitoring/scheduler.py` shows all three jobs.
3. `grep -n 'init_log_tables' backend/monitoring/main.py` shows log tables initialized on startup.
  </verify>
  <done>Log aggregation API endpoints provide search/filter capabilities, error rate tracking, and pattern detection. Scheduler collects errors every 2 minutes, checks error rate every 5 minutes (with alerting), and detects patterns every 30 minutes. Log tables initialized on service startup.</done>
</task>

</tasks>

<verification>
- Error logs are collected from backend application and stored in monitoring database
- Logs can be searched by query text, filtered by level, source, and time range
- Error rate is calculated and tracked over time with per-minute granularity
- Error rate exceeding thresholds triggers alerts through the existing alert pipeline
- Error patterns are detected by normalizing messages and deduplicating
- Patterns can be marked as known/resolved/ignored to reduce noise
- API endpoints serve all log data for dashboard consumption
- Scheduler runs log collection, error rate checking, and pattern detection on intervals
</verification>

<success_criteria>
- Production errors are centrally collected without manual SSH
- Error search returns relevant results within seconds (LIKE-based search on SQLite)
- Error rate tracking detects sustained error spikes (>5 errors/min triggers critical alert)
- Pattern detection groups similar errors together, identifying recurring issues
- Top 20 error patterns visible, sortable by frequency
- New patterns (first seen in 24h) are highlighted as potential new bugs
- Log retention automatically cleaned up at 90 days
</success_criteria>

<output>
After completion, create `.planning/phases/09-monitoring-alerting/09-06-SUMMARY.md`
</output>
